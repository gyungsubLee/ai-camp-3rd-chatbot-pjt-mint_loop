# Trip Kit - Technical Requirements Document (TRD)
## MVP Version 2.0

---

## ğŸ“‹ Document Information

- **Document Version**: 2.0.0
- **Last Updated**: 2025-12-10
- **Project Timeline**: MVP
- **Related Documents**: [PRD_TripKit_MVP.md](./PRD_TripKit_MVP.md), [API_Documentation.md](./API_Documentation.md), [AI_Integration_Guide.md](./AI_Integration_Guide.md)
- **Author**: Engineering Team
- **Status**: Active Development

---

## ğŸ—ï¸ System Architecture Overview

### High-Level Architecture (Vibe-Driven)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Client Layer                                â”‚
â”‚                     (Next.js 14+ App Router)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚  Vibe Chat   â”‚  â”‚  Concept     â”‚  â”‚  Destinations â”‚  â”‚ TripKit â”‚â”‚
â”‚  â”‚  Interface   â”‚  â”‚  Selector    â”‚  â”‚  (SSE Stream) â”‚  â”‚ Package â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“ (API Routes â†’ FastAPI)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Backend Layer (Python FastAPI)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    3-Agent Architecture                        â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚ â”‚
â”‚  â”‚  â”‚  ChatAgent   â”‚  â”‚ Recommendationâ”‚  â”‚  ImageAgent  â”‚       â”‚ â”‚
â”‚  â”‚  â”‚  (LangGraph) â”‚  â”‚    Agent     â”‚  â”‚  (LangGraph) â”‚       â”‚ â”‚
â”‚  â”‚  â”‚  Human-in-   â”‚  â”‚  (LangGraph) â”‚  â”‚  Gemini      â”‚       â”‚ â”‚
â”‚  â”‚  â”‚  the-loop    â”‚  â”‚  SSE Stream  â”‚  â”‚  Imagen      â”‚       â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                     MCP Servers                                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚ â”‚
â”‚  â”‚  â”‚  Search MCP  â”‚  â”‚  Places MCP  â”‚                          â”‚ â”‚
â”‚  â”‚  â”‚  Port: 8050  â”‚  â”‚  Port: 8052  â”‚                          â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     External AI Services                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚   OpenAI     â”‚  â”‚   Gemini     â”‚  â”‚ Google Maps  â”‚              â”‚
â”‚  â”‚   GPT-4o     â”‚  â”‚   Imagen     â”‚  â”‚  Places API  â”‚              â”‚
â”‚  â”‚   (Chat/Rec) â”‚  â”‚  (Images)    â”‚  â”‚              â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Storage Layer (MVP: Session-Based Vibe)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚   Browser    â”‚  â”‚  MemorySaver â”‚                                â”‚
â”‚  â”‚  Vibe State  â”‚  â”‚  (LangGraph) â”‚                                â”‚
â”‚  â”‚  (Zustand)   â”‚  â”‚              â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Architecture Principles

1. **Vibe-First Design**: All recommendations flow from extracted user vibe (mood + aesthetic + interests)
2. **3-Agent Architecture**: ChatAgent, RecommendationAgent, ImageAgent - each with specialized LangGraph workflow
3. **Strategy Pattern**: Provider abstraction for OpenAI/Gemini with dynamic selection
4. **SSE Streaming**: Real-time destination delivery for progressive UI updates
5. **Human-in-the-loop**: ChatAgent interrupts for user input at each conversation step
6. **MCP Integration**: Search MCP for keyword extraction, Places MCP for location enrichment

---

## ğŸ› ï¸ Technology Stack

### Frontend (`front/`)

| Technology | Version | Purpose |
|------------|---------|---------|
| **Next.js** | 14.2+ | React framework, App Router |
| **React** | 18+ | UI component library |
| **TypeScript** | 5.0+ | Type safety |
| **Tailwind CSS** | 3.4+ | Styling framework |
| **Zustand** | 4.5+ | State management (persist middleware) |
| **React Query** | 5.0+ | Data fetching & caching |
| **Framer Motion** | 11.0+ | Animations |

### Backend (`backend/`)

| Technology | Version | Purpose |
|------------|---------|---------|
| **Python** | 3.12+ | Runtime |
| **FastAPI** | 0.115+ | Web framework |
| **LangGraph** | 0.6.4+ | Agent workflow orchestration |
| **LangChain** | 0.3.27+ | AI framework |
| **FastMCP** | 2.11+ | MCP server framework |
| **langchain-mcp-adapters** | 0.1.9+ | MCP integration |
| **Google Generative AI** | 0.8+ | Gemini Imagen integration |
| **OpenAI SDK** | 1.0+ | GPT-4 integration |
| **structlog** | 25.4+ | Structured logging |
| **Pydantic** | 2.0+ | Data validation |

### Infrastructure & DevOps

| Technology | Purpose |
|------------|---------|
| **Vercel** | Frontend hosting, CI/CD |
| **Docker** | Backend containerization |
| **Git/GitHub** | Version control |
| **ESLint/Prettier** | Code quality (Frontend) |
| **Ruff/Black** | Code quality (Backend) |
| **Pytest** | Backend testing |

### External Services

| Service | Purpose | Model |
|---------|---------|-------|
| **OpenAI** | Chat, Recommendations | gpt-4o-mini (default) |
| **Google Gemini** | Image Generation | imagen-3.0-generate-002 |
| **Google Maps** | Location enrichment | Places API |
| **Tavily** | Search (via MCP) | - |

---

## ğŸ“ System Design

### 1. ChatAgent - Vibe Extraction Engine

**Purpose**: Extract user's travel "vibe" through natural conversation using Human-in-the-loop pattern.

#### State Definition (Python)

```python
class ChatState(TypedDict):
    """ChatAgent ìƒíƒœ ì •ì˜"""
    messages: Annotated[list[BaseMessage], add_messages]
    session_id: str
    user_id: str | None
    current_step: ConversationStep
    next_step: ConversationStep
    collected_data: CollectedData
    rejected_items: RejectedItems
    suggested_options: list[str]
    assistant_reply: str
    is_complete: bool
    status: Literal["active", "completed", "error"]

class CollectedData(TypedDict, total=False):
    """ìˆ˜ì§‘ëœ ì‚¬ìš©ì ì„ í˜¸ë„"""
    travel_destination: str    # ì—¬í–‰ ì§€ì—­
    travel_scene: str          # ê¿ˆê¾¸ëŠ” ì—¬í–‰ ì¥ë©´
    travel_companion: str      # ë™í–‰ì
    travel_duration: str       # ì—¬í–‰ ê¸°ê°„
    travel_budget: str         # ì˜ˆì‚°
    travel_style: str          # ì—¬í–‰ ìŠ¤íƒ€ì¼
    special_requests: str      # íŠ¹ë³„ ìš”ì²­
```

#### LangGraph Workflow

```
[START]
  â†“
[process_message] â†’ LLMìœ¼ë¡œ ì‚¬ìš©ì ë©”ì‹œì§€ ì²˜ë¦¬
  â†“
[route_after_process] â†’ ì¡°ê±´ë¶€ ë¶„ê¸°
  â†“
  â”œâ”€â”€ is_complete=True â†’ [finalize] â†’ END
  â”‚
  â””â”€â”€ is_complete=False â†’ END (wait_input)
                          â†‘
                          â”‚ Human-in-the-loop
                          â”‚ (ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°)
                          â†“
                     [Resume with new message]
```

#### Conversation Steps

```python
ConversationStep = Literal[
    "greeting",           # ì¸ì‚¬ ë° ì‹œì‘
    "travel_destination", # ì—¬í–‰ ì§€ì—­
    "travel_scene",       # ê¿ˆê¾¸ëŠ” ì¥ë©´
    "travel_companion",   # ë™í–‰ì
    "travel_duration",    # ê¸°ê°„
    "travel_budget",      # ì˜ˆì‚°
    "travel_style",       # ìŠ¤íƒ€ì¼
    "special_requests",   # íŠ¹ë³„ ìš”ì²­
    "confirmation",       # í™•ì¸
    "complete"           # ì™„ë£Œ
]
```

#### Agent Implementation

```python
class ChatAgent:
    """Human-in-the-loopì„ ì§€ì›í•˜ëŠ” Chat Agent"""

    def __init__(
        self,
        llm_provider: Any = None,
        checkpointer: BaseCheckpointSaver | None = None,
    ):
        # LLM Provider ì„¤ì • (ê¸°ë³¸: Gemini)
        if llm_provider is None:
            from ...providers.gemini_provider import GeminiLLMProvider
            self._llm_provider = GeminiLLMProvider()
        else:
            self._llm_provider = llm_provider

        # Checkpointer ì„¤ì • (ì„¸ì…˜ ìƒíƒœ ì €ì¥)
        self._checkpointer = checkpointer or MemorySaver()

        # ê·¸ë˜í”„ ë¹Œë“œ
        self._graph = self._build_graph()

    async def chat(
        self,
        input_data: ChatInput,
        thread_id: str | None = None,
    ) -> ChatOutput:
        """ëŒ€í™” ì²˜ë¦¬ (ì„¸ì…˜ ë³µêµ¬ ì§€ì›)"""
        session_id = thread_id or input_data["session_id"]
        config = {"configurable": {"thread_id": session_id}}

        # ê¸°ì¡´ ìƒíƒœ í™•ì¸
        existing_state = await self._get_state(session_id)

        if existing_state and existing_state.get("messages"):
            # ê¸°ì¡´ ëŒ€í™” ì¬ê°œ
            result = await self._resume_conversation(...)
        else:
            # ìƒˆ ëŒ€í™” ì‹œì‘
            result = await self._start_conversation(...)

        return self._format_output(result, session_id)
```

---

### 2. RecommendationAgent - Destination Matching

**Purpose**: Generate vibe-matched destination recommendations with SSE streaming.

#### State Definition (Python)

```python
class RecommendationState(TypedDict):
    """RecommendationAgent ìƒíƒœ"""
    messages: Annotated[list[BaseMessage], add_messages]
    user_preferences: dict
    concept: str | None
    travel_scene: str | None
    travel_destination: str | None
    image_generation_context: dict | None
    llm_provider: str
    model: str
    user_profile: dict
    system_prompt: str
    user_prompt: str
    raw_response: str
    destinations: list[dict]
    status: Literal["pending", "processing", "completed", "failed"]
    error: str | None
```

#### LangGraph Workflow

```
[START]
  â†“
[analyze_preferences] â†’ ì‚¬ìš©ì ì„ í˜¸ë„ ë¶„ì„ ë° í”„ë¡œí•„ ìƒì„±
  â†“
[build_prompt] â†’ LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
  â†“
[generate_recommendations] â†’ OpenAI/Geminië¡œ ì¶”ì²œ ìƒì„±
  â†“
[parse_response] â†’ JSON íŒŒì‹± ë° ê²€ì¦
  â†“
[enrich_with_places] â†’ Google Places API ì •ë³´ ë³´ê°•
  â†“
[END]
```

#### SSE Streaming Implementation

```python
async def recommend_stream(
    self,
    input_data: RecommendationInput,
    thread_id: str = "default",
) -> AsyncIterator[dict]:
    """2ë‹¨ê³„ SSE ìŠ¤íŠ¸ë¦¬ë°

    1ë‹¨ê³„: LLM ì‘ë‹µ íŒŒì‹± í›„ ì´ˆê¸° ì—¬í–‰ì§€ ì „ì†¡
    2ë‹¨ê³„: Google Places enrichment í›„ ìƒì„¸ ì •ë³´ ì¶”ê°€
    """
    # === 1ë‹¨ê³„: LLM ì‘ë‹µê¹Œì§€ ì‹¤í–‰ ===
    state = await analyze_preferences_node(state)
    state = await build_prompt_node(state)
    state = await generate_recommendations_node(state, ...)
    state = await parse_response_node(state)

    # === 2ë‹¨ê³„: Places API enrichment (ë³‘ë ¬ ì²˜ë¦¬) ===
    enriched_destinations = await enrich_destinations_parallel(
        state.get("destinations", [])
    )

    # === SSE ìŠ¤íŠ¸ë¦¬ë° ===
    for i, dest in enumerate(enriched_destinations):
        yield {
            "type": "destination",
            "index": i,
            "total": len(enriched_destinations),
            "destination": dest,
        }

    yield {
        "type": "complete",
        "total": len(enriched_destinations),
    }
```

#### Destination Structure

```python
class Destination(TypedDict):
    """ì—¬í–‰ì§€ ì •ë³´"""
    id: str
    name: str
    city: str
    country: str
    description: str
    matchReason: str
    bestTimeToVisit: str
    photographyScore: int  # 1-10
    transportAccessibility: str  # easy | moderate | challenging
    safetyRating: int  # 1-10
    placeDetails: PlaceDetails | None  # Google Places API

class PlaceDetails(TypedDict):
    """Google Places API ì •ë³´"""
    formatted_address: str | None
    geometry: dict | None
    photos: list[str]
    rating: float | None
    user_ratings_total: int | None
    opening_hours: dict | None
    website: str | None
    price_level: int | None
```

---

### 3. ImageAgent - Film Aesthetic Generation

**Purpose**: Generate film-aesthetic preview images using Gemini Imagen.

#### State Definition (Python)

```python
class ImageGenerationState(TypedDict):
    """ImageAgent ìƒíƒœ"""
    messages: Annotated[list[BaseMessage], add_messages]
    user_prompt: str
    extracted_keywords: list[str]
    optimized_prompt: str
    generated_image_url: str | None
    image_metadata: dict | None
    image_model: str
    status: Literal["pending", "extracting", "generating", "completed", "failed"]
    error: str | None
```

#### LangGraph Workflow

```
[START]
  â†“
[extract_keywords] â†’ Search MCPë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
  â†“
[optimize_prompt] â†’ ì´ë¯¸ì§€ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ ìµœì í™”
  â†“
[generate_image] â†’ Gemini Imagenìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„±
  â†“
[END]
```

#### Gemini Imagen Integration

```python
# ê¸°ë³¸ ëª¨ë¸
DEFAULT_IMAGE_MODEL = "imagen-3.0-generate-002"

async def generate_image_node(
    state: ImageGenerationState,
    provider_type: str | None = None,
    image_model: str | None = None
) -> ImageGenerationState:
    """ì´ë¯¸ì§€ ìƒì„± ë…¸ë“œ"""
    actual_model = image_model or state.get("image_model") or DEFAULT_IMAGE_MODEL

    # Provider ê°€ì ¸ì˜¤ê¸° (Strategy Pattern)
    provider = get_provider(provider_type or "gemini", model=actual_model)

    # ì´ë¯¸ì§€ ìƒì„± íŒŒë¼ë¯¸í„°
    params = ImageGenerationParams(
        prompt=state["optimized_prompt"],
        size="1024x1024",
        quality="standard",
        style="vivid"
    )

    # ì´ë¯¸ì§€ ìƒì„± ì‹¤í–‰
    result = await provider.generate(params)

    return {
        **state,
        "generated_image_url": result.url,
        "image_metadata": {
            "provider": result.provider,
            "model": actual_model,
            "revised_prompt": result.revised_prompt,
        },
        "status": "completed"
    }
```

---

### 4. Provider Strategy Pattern

**Purpose**: Abstract LLM/Image providers for flexible switching.

#### Provider Interface

```python
class LLMProvider(ABC):
    """LLM Provider ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤"""

    @property
    @abstractmethod
    def provider_name(self) -> str:
        """í”„ë¡œë°”ì´ë” ì´ë¦„"""
        pass

    @abstractmethod
    async def chat(self, params: ChatParams) -> ChatResult:
        """ì±„íŒ… ì™„ë£Œ"""
        pass

class ImageProvider(ABC):
    """Image Provider ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤"""

    @property
    @abstractmethod
    def provider_name(self) -> str:
        """í”„ë¡œë°”ì´ë” ì´ë¦„"""
        pass

    @abstractmethod
    async def generate(self, params: ImageGenerationParams) -> ImageGenerationResult:
        """ì´ë¯¸ì§€ ìƒì„±"""
        pass
```

#### Provider Factory

```python
def get_provider(
    provider_type: str | None = None,
    **kwargs
) -> LLMProvider | ImageProvider:
    """Provider íŒ©í† ë¦¬ í•¨ìˆ˜

    Args:
        provider_type: "openai" | "gemini"
        **kwargs: í”„ë¡œë°”ì´ë”ë³„ ì¶”ê°€ ì„¤ì •

    Returns:
        ì ì ˆí•œ Provider ì¸ìŠ¤í„´ìŠ¤
    """
    provider_type = provider_type or os.getenv("LLM_PROVIDER", "openai")

    if provider_type == "openai":
        return OpenAIProvider(**kwargs)
    elif provider_type == "gemini":
        return GeminiProvider(**kwargs)
    else:
        raise ValueError(f"Unknown provider: {provider_type}")
```

#### Available Providers

| Provider | Type | Models | Purpose |
|----------|------|--------|---------|
| OpenAI | LLM | gpt-4o, gpt-4o-mini | Chat, Recommendations |
| Gemini | LLM | gemini-2.5-flash | Chat (alternative) |
| Gemini | Image | imagen-3.0-generate-002 | Film aesthetic images |

---

## ğŸ”Œ API Specification

### Base URLs

```
Frontend Dev: http://localhost:3000
Frontend Prod: https://tripkit.vercel.app

Backend Dev: http://localhost:8000
Backend Prod: https://api.tripkit.app
```

### Authentication

**MVP**: No authentication required (public endpoints)
**Future**: JWT-based authentication with Supabase Auth

---

### Endpoints

#### 1. **POST /chat**

**Purpose**: Process conversation and advance chatbot state (session-based)

**Request**:
```json
{
  "message": "ì„œìš¸ì—ì„œ ë‚­ë§Œì ì¸ ì—¬í–‰ì„ í•˜ê³  ì‹¶ì–´ìš”",
  "session_id": "session_abc123",
  "user_id": "user_optional_id"
}
```

**Response**:
```json
{
  "reply": "ì„œìš¸ì—ì„œ ë‚­ë§Œì ì¸ ì—¬í–‰ì„ ê³„íší•˜ì‹œëŠ”êµ°ìš”! ì–´ë–¤ ì¥ë©´ì„ ê¿ˆê¾¸ê³  ê³„ì„¸ìš”?",
  "currentStep": "travel_destination",
  "nextStep": "travel_scene",
  "isComplete": false,
  "collectedData": {
    "travel_destination": "ì„œìš¸"
  },
  "rejectedItems": {},
  "suggestedOptions": ["ì¹´í˜ì—ì„œ ì±… ì½ê¸°", "ì•¼ê²½ ê°ìƒ", "ê³¨ëª©ê¸¸ ì‚°ì±…"]
}
```

#### 2. **GET /chat/history/{session_id}**

**Purpose**: Retrieve conversation history

**Response**:
```json
{
  "history": [
    {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"},
    {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–¤ ì—¬í–‰ì„ ê¿ˆê¾¸ì‹œë‚˜ìš”?"}
  ],
  "sessionId": "session_abc123"
}
```

#### 3. **POST /recommendations/destinations/stream**

**Purpose**: SSE streaming destination recommendations

**Request**:
```json
{
  "preferences": {
    "mood": "romantic",
    "aesthetic": "vintage"
  },
  "concept": "filmlog",
  "travelScene": "ì¹´í˜ì—ì„œ ì±… ì½ê¸°",
  "travelDestination": "ìœ ëŸ½",
  "imageGenerationContext": {
    "filmType": "Kodak Portra 400",
    "cameraModel": "Leica M6"
  }
}
```

**SSE Response**:
```
data: {"type":"destination","index":0,"total":3,"destination":{...}}

data: {"type":"destination","index":1,"total":3,"destination":{...}}

data: {"type":"destination","index":2,"total":3,"destination":{...}}

data: {"type":"complete","total":3,"userProfile":{...}}
```

#### 4. **POST /generate/image**

**Purpose**: Generate film-aesthetic preview image with Gemini Imagen

**Request**:
```json
{
  "prompt": "A cozy Parisian cafe with vintage film aesthetic",
  "concept": "filmlog",
  "filmStock": "kodak_portra_400",
  "style": "cinematic"
}
```

**Response**:
```json
{
  "imageUrl": "https://storage.googleapis.com/...",
  "optimizedPrompt": "A cozy Parisian cafe, warm Kodak Portra 400 tones...",
  "extractedKeywords": ["cafe", "paris", "vintage", "cozy"],
  "metadata": {
    "provider": "gemini",
    "model": "imagen-3.0-generate-002",
    "generationTime": 3500
  },
  "status": "completed"
}
```

---

## ğŸ’¾ Data Models

### Frontend Types (TypeScript)

```typescript
// Core Entities
interface UserPreferences {
  mood: 'romantic' | 'adventurous' | 'nostalgic' | 'peaceful';
  aesthetic: 'urban' | 'nature' | 'vintage' | 'modern';
  duration: 'short' | 'medium' | 'long';
  interests: Interest[];
  concept?: Concept;
}

interface Destination {
  id: string;
  name: string;
  city: string;
  country: string;
  description: string;
  matchReason: string;
  bestTimeToVisit: string;
  photographyScore: number;
  transportAccessibility: 'easy' | 'moderate' | 'challenging';
  safetyRating: number;
  placeDetails?: PlaceDetails;
}

interface ChatResponse {
  reply: string;
  currentStep: ConversationStep;
  nextStep: ConversationStep;
  isComplete: boolean;
  collectedData: CollectedData;
  rejectedItems: RejectedItems;
  suggestedOptions: string[];
}

// Supporting Types
type Interest = 'photography' | 'food' | 'art' | 'history' | 'nature' | 'architecture';
type Concept = 'flaneur' | 'filmlog' | 'midnight';
type ConversationStep =
  | 'greeting' | 'travel_destination' | 'travel_scene'
  | 'travel_companion' | 'travel_duration' | 'travel_budget'
  | 'travel_style' | 'special_requests' | 'confirmation' | 'complete';
```

### Backend Models (Pydantic)

```python
# Request Models
class ChatRequest(BaseModel):
    message: str
    session_id: str
    user_id: str | None = None

class RecommendationRequest(BaseModel):
    preferences: dict[str, Any] = {}
    concept: str | None = None
    travel_scene: str | None = None
    travel_destination: str | None = None
    image_generation_context: dict[str, Any] | None = None

class ImageGenerationRequest(BaseModel):
    prompt: str
    concept: str | None = None
    film_stock: str | None = None
    style: str = "vivid"

# Response Models
class ChatResponse(BaseModel):
    reply: str
    currentStep: str
    nextStep: str
    isComplete: bool
    collectedData: dict[str, Any]
    rejectedItems: dict[str, Any]
    suggestedOptions: list[str]
    sessionId: str

class ImageGenerationResponse(BaseModel):
    imageUrl: str | None
    optimizedPrompt: str
    extractedKeywords: list[str]
    metadata: dict[str, Any]
    status: str
    error: str | None = None
```

---

## ğŸ” Security Considerations

### API Security

1. **Rate Limiting**:
   - 100 requests/hour per IP for chat endpoints
   - 20 image generations/hour per IP
   - SSE streams: 10 concurrent connections per IP

2. **Input Validation**:
   - Pydantic models for all request validation
   - Sanitize user inputs before LLM calls
   - Prevent prompt injection attacks

3. **API Key Protection**:
   - Store all keys in environment variables
   - Never expose keys in client-side code
   - Use separate keys for development/production

4. **CORS**:
   - Restrict to production domain only
   - Development: Allow localhost:3000

### Environment Variables

ëª¨ë“  í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ `.env.sample` íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

ì£¼ìš” ì„¤ì • í•­ëª©:
- `LLM_PROVIDER`: LLM í”„ë¡œë°”ì´ë” ì„ íƒ (openai/gemini)
- `LLM_MODEL`: ì‚¬ìš©í•  LLM ëª¨ë¸
- `IMAGE_PROVIDER`: ì´ë¯¸ì§€ ìƒì„± í”„ë¡œë°”ì´ë”
- `GEMINI_IMAGE_MODEL`: Gemini Imagen ëª¨ë¸
- `SEARCH_MCP_PORT`: Search MCP ì„œë²„ í¬íŠ¸
- `PLACES_MCP_PORT`: Places MCP ì„œë²„ í¬íŠ¸

---

## ğŸš€ Performance Optimization

### Performance Targets

| Metric | Target | Critical? |
|--------|--------|-----------|
| Chat API Response | <3s | Yes |
| SSE First Destination | <5s | Yes |
| SSE Complete (3 destinations) | <10s | Yes |
| Image Generation | <15s | No (nice-to-have) |
| Page Load Time | <3s | Yes |

### Optimization Strategies

#### 1. SSE Streaming for Progressive UI

```python
# Backend: 2-phase streaming
async def recommend_stream(...) -> AsyncIterator[dict]:
    # Phase 1: LLM response (3-5s)
    destinations = await generate_recommendations(...)

    # Phase 2: Places enrichment (parallel, 2-3s)
    enriched = await enrich_destinations_parallel(destinations)

    # Stream each destination
    for dest in enriched:
        yield {"type": "destination", "destination": dest}
```

```typescript
// Frontend: Progressive rendering
const loadDestinationsStream = async () => {
  const response = await fetch("/api/recommendations/destinations/stream", {
    method: "POST",
    body: JSON.stringify({ preferences, concept }),
  });

  const reader = response.body?.getReader();
  const decoder = new TextDecoder();

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const lines = decoder.decode(value).split("\n");
    for (const line of lines) {
      if (line.startsWith("data: ")) {
        const event = JSON.parse(line.slice(6));
        if (event.type === "destination") {
          addDestination(event.destination);  // ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸
        }
      }
    }
  }
};
```

#### 2. Parallel Places API Enrichment

```python
async def enrich_destinations_parallel(
    destinations: list[dict]
) -> list[dict]:
    """ì—¬ëŸ¬ ì—¬í–‰ì§€ë¥¼ ë³‘ë ¬ë¡œ enrichment"""
    tasks = [
        enrich_single_destination(dest)
        for dest in destinations
    ]
    return await asyncio.gather(*tasks)
```

#### 3. Client-Side Caching

```typescript
// Zustand with persist middleware
const useChatStore = create<ChatState>()(
  persist(
    (set, get) => ({
      sessionId: '',
      messages: [],
      collectedData: {},
      // ... actions
    }),
    {
      name: 'tripkit-chat-storage',
      partialize: (state) => ({
        sessionId: state.sessionId,
        collectedData: state.collectedData,
      }),
    }
  )
);
```

#### 4. Session Recovery

```python
# Backend: MemorySaver for session state
class ChatAgent:
    def __init__(self, checkpointer=None):
        self._checkpointer = checkpointer or MemorySaver()

    async def _get_state(self, session_id: str) -> ChatState | None:
        """ì €ì¥ëœ ìƒíƒœ ì¡°íšŒ"""
        config = {"configurable": {"thread_id": session_id}}
        snapshot = await self._graph.aget_state(config)
        return snapshot.values if snapshot else None
```

---

## ğŸ§ª Testing Strategy

### Testing Pyramid

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    E2E     â”‚  10% - Critical user flows
        â”‚   Tests    â”‚  (Playwright)
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Integration  â”‚  30% - API endpoints, agents
       â”‚    Tests     â”‚  (Pytest + httpx)
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Unit Tests    â”‚  60% - Nodes, utilities
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (Pytest + Jest)
```

### Backend Tests (Pytest)

```python
# tests/test_chat_agent.py
import pytest
from src.agents.chat_agent import ChatAgent

@pytest.fixture
def chat_agent():
    return ChatAgent()

@pytest.mark.asyncio
async def test_new_conversation(chat_agent):
    """ìƒˆ ëŒ€í™” ì‹œì‘ í…ŒìŠ¤íŠ¸"""
    result = await chat_agent.chat({
        "message": "ì•ˆë…•í•˜ì„¸ìš”",
        "session_id": "test_session_1"
    })

    assert result["reply"]
    assert result["currentStep"] == "greeting"
    assert result["isComplete"] is False

@pytest.mark.asyncio
async def test_session_recovery(chat_agent):
    """ì„¸ì…˜ ë³µêµ¬ í…ŒìŠ¤íŠ¸"""
    # ì²« ë²ˆì§¸ ë©”ì‹œì§€
    await chat_agent.chat({
        "message": "ì„œìš¸ì—ì„œ ì—¬í–‰í•˜ê³  ì‹¶ì–´ìš”",
        "session_id": "test_session_2"
    })

    # ë‘ ë²ˆì§¸ ë©”ì‹œì§€ (ì„¸ì…˜ ì¬ê°œ)
    result = await chat_agent.chat({
        "message": "ì¹´í˜ì—ì„œ ì±… ì½ê¸°",
        "session_id": "test_session_2"  # ë™ì¼í•œ ì„¸ì…˜ ID
    })

    assert "travel_destination" in result["collectedData"]
```

### Frontend Tests (Jest)

```typescript
// tests/components/ChatContainer.test.tsx
import { render, screen, waitFor } from '@testing-library/react';
import userEvent from '@testing-library/user-event';
import ChatContainer from '@/components/chat/ChatContainer';

describe('ChatContainer', () => {
  it('should send message and display AI response', async () => {
    render(<ChatContainer />);

    const input = screen.getByPlaceholderText('ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”');
    await userEvent.type(input, 'ì„œìš¸ì—ì„œ ì—¬í–‰í•˜ê³  ì‹¶ì–´ìš”');
    await userEvent.click(screen.getByRole('button', { name: /send/i }));

    await waitFor(() => {
      expect(screen.getByText(/ì–´ë–¤ ì¥ë©´ì„ ê¿ˆê¾¸/)).toBeInTheDocument();
    });
  });
});
```

---

## ğŸ“¦ Deployment

### Docker Deployment (Backend)

```dockerfile
# backend/Dockerfile
FROM python:3.12-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY src/ ./src/

EXPOSE 8000

CMD ["uvicorn", "src.api_server.server:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    env_file:
      - .env
    restart: unless-stopped

  search-mcp:
    build: ./backend
    command: python -m src.mcp_servers.search_server
    ports:
      - "8050:8050"
    env_file:
      - .env

  places-mcp:
    build: ./backend
    command: python -m src.mcp_servers.places_server
    ports:
      - "8052:8052"
    env_file:
      - .env
```

### Vercel Deployment (Frontend)

```json
{
  "version": 2,
  "builds": [
    {
      "src": "package.json",
      "use": "@vercel/next"
    }
  ],
  "env": {
    "BACKEND_URL": "@backend-url",
    "AGENT_API_URL": "@agent-api-url"
  }
}
```

---

## ğŸ“Š Monitoring & Observability

### Structured Logging (Backend)

```python
import structlog

logger = structlog.get_logger(__name__)

# Agent ë¡œê¹…
logger.info(
    "Starting recommendation generation",
    concept=input_data.get("concept"),
    destination=input_data.get("travel_destination"),
    provider=actual_provider,
    model=actual_model
)

# ì—ëŸ¬ ë¡œê¹…
logger.error(
    "Chat processing error",
    session_id=session_id,
    error=str(e),
)
```

### Frontend Analytics

```typescript
// app/layout.tsx
import { Analytics } from '@vercel/analytics/react';

export default function RootLayout({ children }) {
  return (
    <html>
      <body>
        {children}
        <Analytics />
      </body>
    </html>
  );
}
```

---

## ğŸ”„ CI/CD Pipeline

### GitHub Actions

CI/CD íŒŒì´í”„ë¼ì¸ì€ GitHub Actionsë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì„±í•©ë‹ˆë‹¤.

**ì£¼ìš” ì›Œí¬í”Œë¡œìš°**:

1. **í…ŒìŠ¤íŠ¸ (Frontend)**:
   - Node.js 20 ì„¤ì •
   - npm ci, lint, type-check, test ì‹¤í–‰

2. **í…ŒìŠ¤íŠ¸ (Backend)**:
   - Python 3.12 ì„¤ì •
   - pytest ì‹¤í–‰

3. **ë°°í¬ (Frontend)**:
   - Vercel Actionì„ ì‚¬ìš©í•˜ì—¬ ìë™ ë°°í¬
   - main ë¸Œëœì¹˜ push ì‹œ í”„ë¡œë•ì…˜ ë°°í¬

4. **í™˜ê²½ ë³€ìˆ˜**:
   - GitHub Repository Settingsì—ì„œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
   - Vercel í†µí•©ì„ í†µí•œ ìë™ í™˜ê²½ ë³€ìˆ˜ ì£¼ì…

---

## ğŸ“š External Dependencies

### Required Services

| Service | Purpose | Pricing |
|---------|---------|---------|
| **OpenAI GPT-4o-mini** | Chat, Recommendations | ~$0.15/1M input tokens |
| **Google Gemini Imagen** | Image generation | Usage-based |
| **Google Maps Places** | Location enrichment | $17/1K requests |
| **Tavily Search** | Keyword extraction | Free tier available |

### Python Dependencies

```
# requirements.txt
fastapi>=0.115.0
uvicorn[standard]>=0.32.0
langgraph>=0.6.4
langchain>=0.3.27
langchain-openai>=0.3.18
langchain-google-genai>=2.1.5
fastmcp>=2.11.0
langchain-mcp-adapters>=0.1.9
google-generativeai>=0.8.5
openai>=1.0.0
pydantic>=2.0.0
structlog>=25.4.0
httpx>=0.28.0
python-dotenv>=1.0.0
```

---

## ğŸ”® Future Enhancements (Post-MVP)

### Phase 2

- User authentication (Supabase Auth)
- PostgresSaver for persistent sessions
- User profile and preferences storage
- Share functionality (social media, export PDF)

### Phase 3

- Redis caching for API responses
- O2O rental system integration
- Payment processing (Stripe)
- Mobile app (React Native)

---

## ğŸ“ Appendix

### A. Glossary

- **LangGraph**: State machine framework for AI agent workflows
- **Human-in-the-loop**: Pattern where workflow pauses for user input
- **SSE (Server-Sent Events)**: One-way real-time communication from server to client
- **MCP (Model Context Protocol)**: Standard for AI tool integration
- **Strategy Pattern**: Design pattern for interchangeable algorithms (LLM providers)
- **Film Aesthetic**: Visual style mimicking analog film cameras

### B. Reference Links

- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Next.js Documentation](https://nextjs.org/docs)
- [Gemini Imagen API](https://cloud.google.com/vertex-ai/generative-ai/docs/image/overview)
- [OpenAI API Documentation](https://platform.openai.com/docs)

### C. Revision History

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.0.0 | 2025-12-03 | Initial TRD for MVP | Engineering Team |
| 2.0.0 | 2025-12-10 | Updated to reflect actual implementation: 3-Agent Architecture, Python backend, Gemini Imagen, SSE streaming | Engineering Team |

---

**Document Status**: âœ… Ready for Development Reference
